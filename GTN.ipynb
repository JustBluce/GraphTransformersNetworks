{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Package Install\n",
    "- Please use the command below to down these Python package if they are not installed in your virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.8.0\n",
    "!pip install numpy==1.22.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caizf/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"DBLP\" # DBLP ACM\n",
    "num_layers = 3\n",
    "epochs = 4\n",
    "node_dim = 64\n",
    "num_channels = 2\n",
    "norm=True\n",
    "lr = 0.005\n",
    "weight_decay = 0.001\n",
    "adaptive_lr = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download\n",
    "Please download datasets (DBLP, ACM, IMDB) from this [link](https://drive.google.com/file/d/13eC9gz8b9mLCPC_V1iHXJSKNHCTrTOYa/view?usp=sharing) and extract data.zip into data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1722/4016221159.py:4: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n",
      "/tmp/ipykernel_1722/4016221159.py:4: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "with open('data/' + dataset + '/node_features.pkl', 'rb') as f:\n",
    "    node_features = pickle.load(f)\n",
    "with open('data/' + dataset+'/edges.pkl','rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "with open('data/' + dataset+'/labels.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "num_nodes = edges[0].shape[0] # num_nodes\n",
    "\n",
    "for i, edge in enumerate(edges):\n",
    "    if i ==0:\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        A = torch.cat([A, torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "# A: [num_nodes, num_nodes, num_edges]\n",
    "\n",
    "A = torch.cat([A, torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1) # A: [num_nodes, num_nodes, num_edges + 1]\n",
    "\n",
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor) # node_features: [num_nodes, node_embedding_input]\n",
    "train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor) # 800\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor) # 800\n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor) # 400\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor) # 400\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor) # 2857\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor) # 2857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transformers Neyworks Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class GTN(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The module to Compose Graph Transformers Neural Network .\n",
    "\n",
    "    Input shape:\n",
    "        A [num_nodes, num_nodes, num_edges + 1]\n",
    "        X [num_nodes, node_embedding_input]\n",
    "        target_x [num_target_node] DBLP 400\n",
    "        target [num_target_node] DBLP 400\n",
    "\n",
    "    Output shape:\n",
    "        y: [num_target_nodes, num_node_classes]\n",
    "        Ws: A set of [num_channels, num_edges + 1, 1, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class ,num_layers, norm):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge # num_edge_classes 4\n",
    "        self.num_channels = num_channels # num_channelsn 2\n",
    "        self.w_in = w_in # node_embedding_input 334\n",
    "        self.w_out = w_out # node_embedding_output 64\n",
    "        self.num_class = num_class # num_node_classes 4\n",
    "        self.num_layers = num_layers # num_layers 3\n",
    "        self.is_norm = norm\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(GraphTransformersLayer(num_edge, num_channels, first=True))\n",
    "            else:\n",
    "                layers.append(GraphTransformersLayer(num_edge, num_channels, first=False))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.weight = nn.Parameter(torch.Tensor(w_in, w_out)) # [node_embedding_input, node_embedding_output]\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.bias = nn.Parameter(torch.Tensor(w_out)) # [node_embedding_output]\n",
    "        nn.init.zeros_(self.bias)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.linear1 = nn.Linear(self.w_out*num_channels, self.w_out) # [node_embedding_output * num_channels, node_embedding_output]\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class) # [node_embedding_output, num_node_classes]\n",
    "\n",
    "    def gcn_conv(self, X, H):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            X: [num_nodes, node_embedding_input]\n",
    "            H: [num_nodes, num_nodes]\n",
    "\n",
    "        Middle variant:\n",
    "            X: [num_nodes, node_embedding_output]\n",
    "            H: [num_nodes, num_nodes]\n",
    "\n",
    "        Output shape:\n",
    "            [num_nodes, node_embedding_output]\n",
    "        \"\"\"\n",
    "        \n",
    "        X = torch.mm(X, self.weight) # [num_nodes, node_embedding_output]\n",
    "        H = self.norm(H, add=True)  # [num_nodes, num_nodes]\n",
    "        return torch.mm(H.t(),X)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            H: [num_channels, num_nodes, num_nodes]\n",
    "\n",
    "        Output shape:\n",
    "            H_: [num_channels, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                H_ = self.norm(H[i, :, :]).unsqueeze(0)\n",
    "            else:\n",
    "                H_ = torch.cat((H_, self.norm(H[i, :, :]).unsqueeze(0)), dim=0)\n",
    "        return H_\n",
    "\n",
    "    def norm(self, H, add=False):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            H: [num_nodes, num_nodes]\n",
    "\n",
    "        Output shape:\n",
    "            H_: [num_channels, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "\n",
    "        H = H.t()\n",
    "\n",
    "        if add == False:\n",
    "            H = H * ((torch.eye(H.shape[0])==0).type(torch.FloatTensor)) # H: [num_nodes, num_nodes]\n",
    "        else:\n",
    "            H = H * ((torch.eye(H.shape[0])==0).type(torch.FloatTensor)) + torch.eye(H.shape[0]).type(torch.FloatTensor) # H: [num_nodes, num_nodes]\n",
    "\n",
    "        deg = torch.sum(H, dim=1) # deg: [num_nodes]\n",
    "        deg_inv = deg.pow(-1) # deg_inv: [num_nodes]\n",
    "        deg_inv[deg_inv == float('inf')] = 0 # deg_inv: [num_nodes]\n",
    "        deg_inv = deg_inv * torch.eye(H.shape[0]).type(torch.FloatTensor) # deg_inv: [num_nodes, num_nodes]\n",
    "        H = torch.mm(deg_inv, H) # deg_inv: [num_nodes, num_nodes]\n",
    "        H = H.t() # H: [num_nodes, num_nodes]\n",
    "        return H\n",
    "\n",
    "    def forward(self, A, X, target_x, target):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            A [num_nodes, num_nodes, num_edges + 1]\n",
    "            X [num_nodes, node_embedding_input]\n",
    "            target_x [num_target_node]\n",
    "            target [num_target_node]\n",
    "\n",
    "        Output shape:\n",
    "            y: [num_target_nodes, num_node_classes]\n",
    "            Ws: A set of [num_channels, num_edges + 1, 1, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        A = A.unsqueeze(0).permute(0,3,1,2)  # [1, num_edges + 1, num_nodes, num_nodes]\n",
    "        Ws = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                H, W = self.layers[i](A) # H: [num_channels, num_nodes, num_nodes]\n",
    "            else:\n",
    "                H = self.normalization(H) # H: [num_channels, num_nodes, num_nodes]\n",
    "                H, W = self.layers[i](A, H) # H: [num_channels, num_nodes, num_nodes]\n",
    "            Ws.append(W)\n",
    "        \n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                # X: [num_nodes, node_embedding_input] H[i]: [num_nodes, num_nodes]\n",
    "                X_ = F.relu(self.gcn_conv(X, H[i])) # X_: [num_nodes, node_embedding_output]\n",
    "            else:\n",
    "                # X: [num_nodes, node_embedding_input] H[i]: [num_nodes, num_nodes]\n",
    "                X_tmp = F.relu(self.gcn_conv(X, H[i])) # X_tmp: [num_nodes, node_embedding_output]\n",
    "                X_ = torch.cat((X_,X_tmp), dim=1) # X_: [num_nodes, node_embedding_output * num_channels]\n",
    "        X_ = self.linear1(X_) # X_: [num_nodes, node_embedding_output]\n",
    "        X_ = F.relu(X_) # X_: [num_nodes, node_embedding_output]\n",
    "        y = self.linear2(X_[target_x]) # [num_target_nodes, num_node_classes]\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws\n",
    "\n",
    "\n",
    "class GraphTransformersLayer(nn.Module):\n",
    "    '''\n",
    "    The module to Compose Graph Transformers Convolutional Neural Network Layer.\n",
    "    This module is to finsh convilution of two layers.\n",
    "    After being sent to this module, the meta-path will be extended by one unitã€‚\n",
    "\n",
    "    Input shape:\n",
    "        A: [1, num_edges + 1, num_nodes, num_nodes]\n",
    "        H: [num_channels, num_nodes, num_nodes]\n",
    "\n",
    "    Output shape:\n",
    "        H: [num_channels, num_nodes, num_nodes]\n",
    "        W: [num_channels, num_edges + 1, 1, 1]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, first=True):\n",
    "        super(GraphTransformersLayer, self).__init__()\n",
    "\n",
    "        # Parameter Setting\n",
    "        self.in_channels = in_channels # num_edges\n",
    "        self.out_channels = out_channels # num_channels\n",
    "        self.first = first\n",
    "\n",
    "        if self.first == True:\n",
    "            self.conv1 = GraphTransformersConvModule(in_channels, out_channels)\n",
    "            self.conv2 = GraphTransformersConvModule(in_channels, out_channels)\n",
    "        else:\n",
    "            self.conv1 = GraphTransformersConvModule(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, A, H_=None): # A: [1, num_edges + 1, num_nodes, num_nodes] H: [num_channels, num_nodes, num_nodes]\n",
    "        if self.first == True:\n",
    "            a = self.conv1(A) # [num_channels, num_nodes, num_nodes]\n",
    "            b = self.conv2(A) # [num_channels, num_nodes, num_nodes]\n",
    "            H = torch.bmm(a, b) # [num_channels, num_nodes, num_nodes]\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach(),(F.softmax(self.conv2.weight, dim=1)).detach()]\n",
    "        else:\n",
    "            a = self.conv1(A) # [num_channels, num_nodes, num_nodes]\n",
    "            H = torch.bmm(H_, a) # [num_channels, num_nodes, num_nodes]\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n",
    "        return H, W\n",
    "\n",
    "class GraphTransformersConvModule(nn.Module):\n",
    "    '''\n",
    "    The module to do Graph Transformers Convolutional Neural Network.\n",
    "    This is a backbone module to finish edge classes to channels.\n",
    "\n",
    "    Input shape:\n",
    "        A: [1, num_edges + 1, num_nodes, num_nodes]\n",
    "\n",
    "    Output shape:\n",
    "        A: [num_channels, num_nodes, num_nodes]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphTransformersConvModule, self).__init__()\n",
    "\n",
    "        # Module Parameters Setting\n",
    "        self.in_channels = in_channels # num_edges + 1\n",
    "        self.out_channels = out_channels # num_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels).unsqueeze(2).unsqueeze(3)) # [num_channels, num_edges + 1, 1, 1]\n",
    "        nn.init.constant_(self.weight, 0.1)\n",
    "        self.bias = None\n",
    "        # print(\"GraphTransformersConvModule Start\")\n",
    "\n",
    "    def forward(self, A): # A: [1, num_edges + 1, num_nodes, num_nodes]\n",
    "        A = torch.sum(A * F.softmax(self.weight, dim=1), dim=1) # A: [num_channels, num_nodes, num_nodes]\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "    return out\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "    return out\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 1.386662483215332, Macro_F1: 0.19890807569026947\n",
      "Valid - Loss: 1.351372241973877, Macro_F1: 0.29231950640678406\n",
      "Test - Loss: 1.3450151681900024, Macro_F1: 0.2843223214149475\n",
      "\n",
      "Epoch: 1\n",
      "Train - Loss: 1.3486584424972534, Macro_F1: 0.29405418038368225\n",
      "Valid - Loss: 1.2783275842666626, Macro_F1: 0.8466583490371704\n",
      "Test - Loss: 1.2830886840820312, Macro_F1: 0.8195434212684631\n",
      "\n",
      "Epoch: 2\n",
      "Train - Loss: 1.274963140487671, Macro_F1: 0.8568823933601379\n",
      "Valid - Loss: 1.1668626070022583, Macro_F1: 0.7784095406532288\n",
      "Test - Loss: 1.1480616331100464, Macro_F1: 0.7526983618736267\n",
      "\n",
      "Epoch: 3\n",
      "Train - Loss: 1.1701091527938843, Macro_F1: 0.741525411605835\n",
      "Valid - Loss: 1.06666898727417, Macro_F1: 0.671305239200592\n",
      "Test - Loss: 1.1062723398208618, Macro_F1: 0.6496049761772156\n",
      "\n",
      "Train - Loss: 1.3486584424972534, Macro_F1: 0.29405418038368225\n",
      "Valid - Loss: 1.2783275842666626, Macro_F1: 0.8466583490371704\n",
      "Test - Loss: 1.2830886840820312, Macro_F1: 0.8195434212684631\n"
     ]
    }
   ],
   "source": [
    "num_classes = torch.max(train_target).item() + 1\n",
    "final_f1 = 0\n",
    "for l in range(1):\n",
    "    model = GTN(num_edge=A.shape[-1], # num_edges 4\n",
    "                        num_channels=num_channels, # num_channels 2\n",
    "                        w_in = node_features.shape[1], # node_embedding_input 334\n",
    "                        w_out = node_dim,  # node_embedding_output 64\n",
    "                        num_class=num_classes, # num_classes 4\n",
    "                        num_layers=num_layers, # num_layers 3\n",
    "                        norm=norm)\n",
    "    if adaptive_lr == 'false':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{'params':model.weight},\n",
    "                                    {'params':model.linear1.parameters()},\n",
    "                                    {'params':model.linear2.parameters()},\n",
    "                                    {\"params\":model.layers.parameters(), \"lr\":0.5}\n",
    "                                    ], lr=0.005, weight_decay=0.001)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # Train & Valid & Test\n",
    "    best_val_loss = 2000\n",
    "    best_test_loss = 20000\n",
    "    best_train_loss = 20000\n",
    "    best_train_f1 = 0\n",
    "    best_val_f1 = 0\n",
    "    best_test_f1 = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # To make sure that the learning rate is limited in a rellatively small range.\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if param_group['lr'] > 0.005:\n",
    "                param_group['lr'] = param_group['lr'] * 0.9\n",
    "\n",
    "        print('Epoch: {}'.format(i))\n",
    "\n",
    "        # Clean the gradient\n",
    "        model.zero_grad()\n",
    "        model.train()\n",
    "\n",
    "        # Training\n",
    "        loss, y_train, Ws = model(A, node_features, train_node, train_target) # \n",
    "        train_f1 = torch.mean(f1_score(torch.argmax(y_train.detach(), dim=1), train_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Train - Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "\n",
    "        # gradient backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            loss_val, y_valid, W_val = model.forward(A, node_features, valid_node, valid_target)\n",
    "            val_f1 = torch.mean(f1_score(torch.argmax(y_valid, dim=1), valid_target, num_classes=num_classes)).cpu().numpy()\n",
    "            print('Valid - Loss: {}, Macro_F1: {}'.format(loss_val.detach().cpu().numpy(), val_f1))\n",
    "\n",
    "            loss_test, y_test, W_test = model.forward(A, node_features, test_node, test_target)\n",
    "            test_f1 = torch.mean(f1_score(torch.argmax(y_test,dim=1), test_target, num_classes=num_classes)).cpu().numpy()\n",
    "            print('Test - Loss: {}, Macro_F1: {}\\n'.format(loss_test.detach().cpu().numpy(), test_f1))\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_loss = loss_val.detach().cpu().numpy()\n",
    "            best_test_loss = loss_test.detach().cpu().numpy()\n",
    "            best_train_loss = loss.detach().cpu().numpy()\n",
    "            best_train_f1 = train_f1\n",
    "            best_val_f1 = val_f1\n",
    "            best_test_f1 = test_f1 \n",
    "            \n",
    "    print('Train - Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "    print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "    print('Test - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_test_f1))\n",
    "    final_f1 += best_test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://arxiv.org/pdf/1911.06455v2.pdf\n",
    "- https://github.com/seongjunyun/Graph_Transformer_Networks\n",
    "- https://www.youtube.com/watch?v=91yBCJIkpLc&t=304s\n",
    "- https://distill.pub/2021/gnn-intro/\n",
    "- https://distill.pub/2021/understanding-gnns/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1cda7e7263ddca0899cb384315748bf9c15cb26a0071430e60d15e233b50c48"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
